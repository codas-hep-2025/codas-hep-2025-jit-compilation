{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882b9184",
   "metadata": {},
   "source": [
    "# JIT-compilation with Numba and JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d159d",
   "metadata": {},
   "source": [
    "Let's consider the following quadratic formula:\n",
    "```python\n",
    "def quadratic_formula(a, b, c):\n",
    "    return (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e580ecb",
   "metadata": {},
   "source": [
    "\n",
    "What does this computation do? It gets more clear if we write this computation out:\n",
    "```python\n",
    "def pedantic_quadratic_formula(a, b, c):\n",
    "    tmp1 = np.negative(b)            # -b\n",
    "    tmp2 = np.square(b)              # b**2\n",
    "    tmp3 = np.multiply(4, a)         # 4*a\n",
    "    tmp4 = np.multiply(tmp3, c)      # tmp3*c\n",
    "    del tmp3\n",
    "    tmp5 = np.subtract(tmp2, tmp4)   # tmp2 - tmp4\n",
    "    del tmp2, tmp4\n",
    "    tmp6 = np.sqrt(tmp5)             # sqrt(tmp5)\n",
    "    del tmp5\n",
    "    tmp7 = np.add(tmp1, tmp6)        # tmp1 + tmp6\n",
    "    del tmp1, tmp6\n",
    "    tmp8 = np.multiply(2, a)         # 2*a\n",
    "    return np.divide(tmp7, tmp8)     # tmp7 / tmp8\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48703e94",
   "metadata": {},
   "source": [
    "\n",
    "There are **9(!)** elementwise operations that each runs a compiled loop, i.e.:\n",
    "```python\n",
    "tmp1 = np.negative(b)  \n",
    "tmp2 = np.square(b)\n",
    "...\n",
    "\n",
    "# is equivalent to\n",
    "n = len(b)\n",
    "tmp1 = np.empty(n)\n",
    "for i in range(n):  # (compiled loop)\n",
    "    tmp1[i] = -b[i]\n",
    "\n",
    "tmp2 = np.empty(n)\n",
    "for i in range(n):  # (compiled loop)\n",
    "    tmp2[i] = b[i] ** 2\n",
    "\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c1c6e",
   "metadata": {},
   "source": [
    "\n",
    "It would be much more efficient to apply all elementwise operations in a single loop:\n",
    "```python\n",
    "n = len(b)\n",
    "out = np.empty(n)\n",
    "for i in range(n):  # (compiled loop)\n",
    "    out[i] = (-b[i] + np.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6fdc85",
   "metadata": {},
   "source": [
    "Essentially, we're able to get rid of _intermediate_ arrays by \"fusing operations\" using just-in-time (JIT) compilation by applying these operations in a _single_ iteration over our data.\n",
    "\n",
    "Fusing operations is a tricky task however. There are a few ways to achieve this for array processing in Python, and I'd like to highlight two of them:\n",
    "\n",
    "- Numba: https://numba.pydata.org\n",
    "- JAX: https://github.com/jax-ml/jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9914c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# Numba\n",
    "import numba as nb\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a37b3",
   "metadata": {},
   "source": [
    "Let's consider the quadratic formula example again, and compare the runtimes for NumPy, Numba, and JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6227aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data\n",
    "a = np.random.uniform(5, 10, 5_000_000)\n",
    "b = np.random.uniform(10, 20, 5_000_000)\n",
    "c = np.random.uniform(-0.1, 0.1, 5_000_000)\n",
    "\n",
    "\n",
    "# Setup quadratic formula\n",
    "def quadratic_formula(a, b, c):\n",
    "    return (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e56fd",
   "metadata": {},
   "source": [
    "NumPy case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab610b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r3\n",
    "\n",
    "quadratic_formula(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d0563",
   "metadata": {},
   "source": [
    "Numba case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdebf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit  # JIT compile!\n",
    "def quadratic_formula_numba(a, b, c):\n",
    "    n = a.shape[0]\n",
    "    out = np.empty(n)\n",
    "    for i in range(n):\n",
    "        out[i] = (-b[i] + np.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1\n",
    "\n",
    "quadratic_formula_numba(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10 -r3\n",
    "\n",
    "quadratic_formula_numba(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b672a8",
   "metadata": {},
   "source": [
    "JAX case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data\n",
    "a_jax = jnp.asarray(a)\n",
    "b_jax = jnp.asarray(b)\n",
    "c_jax = jnp.asarray(c)\n",
    "\n",
    "\n",
    "@jax.jit  # JIT compile!\n",
    "def quadratic_formula_jax(a, b, c):\n",
    "    return (-b + jnp.sqrt(b**2 - 4*a*c)) / (2*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1\n",
    "\n",
    "quadratic_formula_jax(a_jax, b_jax, c_jax).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410130fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10 -r3\n",
    "\n",
    "quadratic_formula_jax(a_jax, b_jax, c_jax).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d047444",
   "metadata": {},
   "source": [
    "The first invocation for JAX & Numba took longer than consecutive ones. That's the compile time! Afterwards the compiled function is cached...\n",
    "\n",
    "But JAX is still much faster, why?\n",
    "\n",
    "One important difference is that JAX uses as many threads as it has access to. Numba is single-threaded, but can be multithreaded using `parallel=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(parallel=True)  # JIT compile with `parallel=True`!\n",
    "def quadratic_formula_numba_parallel(a, b, c):\n",
    "    n = a.shape[0]\n",
    "    out = np.empty(n)\n",
    "    for i in nb.prange(n):  # note: `range` -> `nb.prange`\n",
    "        out[i] = (-b[i] + np.sqrt(b[i]**2 - 4*a[i]*c[i])) / (2*a[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1\n",
    "\n",
    "quadratic_formula_numba_parallel(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10 -r3\n",
    "\n",
    "quadratic_formula_numba_parallel(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a60ef",
   "metadata": {},
   "source": [
    "Now we're roughly on-par with JAX and Numba with ~2-3ms runtime compared to NumPy's ~23ms.\n",
    "\n",
    "\n",
    "You might have noticed a fundamental difference between JAX and Numba in how those kernels are written: \n",
    "\n",
    "- Numba forces[<sup id=\"fn1-back\">1</sup>](#fn1) you to write _imperative_ code\n",
    "- JAX forces[<sup id=\"fn2-back\">2</sup>](#fn2) you to write _array-oriented_ code\n",
    "\n",
    "\n",
    "![image](https://raw.githubusercontent.com/jpivarski-talks/2023-12-18-hsf-india-tutorial-bhubaneswar/refs/heads/main/img/slow-fast-imperative-vectorized.svg)\n",
    "\n",
    "\n",
    "\n",
    "[<sup id=\"fn1\">1</sup>](#fn1-back) <sup>Can be written array-oriented with `nb.vectorize`.</sup> \n",
    "\n",
    "[<sup id=\"fn2\">2</sup>](#fn2-back) <sup>Can be written imperative with JAX's own loop primitives, e.g. `jax.lax.scan`.</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c850a5",
   "metadata": {},
   "source": [
    "### How does JIT compilation even work? (JAX)\n",
    "\n",
    "Let's have a look at the JAX example, what does `jax.jit` do?\n",
    "\n",
    "It works in 4 steps:\n",
    "1. Stage out a `jax.jit`-decorated function into a new program using a JAX internal IR (JaxPr)\n",
    "2. Lower this IR (JaxPr) into the StableHLO IR\n",
    "3. Compile the StableHLO program with the XLA compiler\n",
    "4. Execute the compiled program\n",
    "\n",
    "Let's see those 4 steps in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the JaxPr (through tracing)\n",
    "traced = quadratic_formula_jax.trace(a_jax, b_jax, c_jax)\n",
    "print(traced.jaxpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d619151",
   "metadata": {},
   "source": [
    "This JaxPr looks a lot like the previously shown pedantic version of the quadratic formula (lecture part-2):\n",
    "\n",
    "```python\n",
    "def pedantic_quadratic_formula(a, b, c):\n",
    "    tmp1 = np.negative(b)            # -b\n",
    "    tmp2 = np.square(b)              # b**2\n",
    "    tmp3 = np.multiply(4, a)         # 4*a\n",
    "    tmp4 = np.multiply(tmp3, c)      # tmp3*c\n",
    "    del tmp3\n",
    "    tmp5 = np.subtract(tmp2, tmp4)   # tmp2 - tmp4\n",
    "    del tmp2, tmp4\n",
    "    tmp6 = np.sqrt(tmp5)             # sqrt(tmp5)\n",
    "    del tmp5\n",
    "    tmp7 = np.add(tmp1, tmp6)        # tmp1 + tmp6\n",
    "    del tmp1, tmp6\n",
    "    tmp8 = np.multiply(2, a)         # 2*a\n",
    "    return np.divide(tmp7, tmp8)     # tmp7 / tmp8\n",
    "```\n",
    "\n",
    "But instead of executing line-by-line we'll lower our JaxPr to StableHLO, and then compile it with XLA to fuse those kernels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4cfe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Lower the JaxPr to StableHLO (still looks similar to our pedantic code)\n",
    "lowered = quadratic_formula_jax.lower(a_jax, b_jax, c_jax)\n",
    "print(lowered.as_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef8ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compile the StableHLO program with XLA\n",
    "compiled = lowered.compile()\n",
    "# print(compiled.as_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Execute the compiled program\n",
    "print(compiled(a_jax, b_jax, c_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9ef0f",
   "metadata": {},
   "source": [
    "### Limitations of Numba\n",
    "\n",
    "You can not JIT-compile arbitrary Python functions. Numba can only JIT-compile a subset of Python, i.e. everything that's \"known\" to Numba as a type (mostly NumPy & NumPy operations).\n",
    "\n",
    "For more information, see: https://numba.readthedocs.io/en/stable/user/5minguide.html#will-numba-work-for-my-code.\n",
    "\n",
    "\n",
    "Check the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def sum_dict_values(d):\n",
    "    out = 0.\n",
    "    for v in d.values():\n",
    "        out += v\n",
    "    return out\n",
    "\n",
    "sum_dict_values({\"a\": 1.0, \"b\": 2.0, \"c\": 3.0})  # Fails, because `dict` is not a known type for Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ff828",
   "metadata": {},
   "source": [
    "### Limitations of JAX\n",
    "\n",
    "JAX infers the operations that are going to be run through a \"tracing step\". Essentially, JAX will run your program once with shallow array objects (no data, just metadata). That let's you JIT-compile all of Python, **but** you can't JIT-compile data-dependent operations.\n",
    "\n",
    "For more \"sharp bits\", see: https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html.\n",
    "\n",
    "Check the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-dependent operations are not traceable\n",
    "\n",
    "@jax.jit\n",
    "def accumulate_if(arr):\n",
    "    print(arr)\n",
    "    if jnp.any(arr > 3):\n",
    "        return jnp.sum(arr)\n",
    "    else:\n",
    "        return jnp.prod(arr)\n",
    "\n",
    "\n",
    "array = jnp.array([1., 2., 3., 4., 5.])\n",
    "print(accumulate_if(array))  # Fails, because jnp.any(arr > 3) is not traceable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be44fdc",
   "metadata": {},
   "source": [
    "Another limitation of JAX is that you can't JIT compile programs with unknown shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4cac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sum_greater_than_three(arr):\n",
    "    return jnp.sum(arr[arr > 3.0])\n",
    "\n",
    "\n",
    "array = jnp.array([1., 2., 3., 4., 5.])\n",
    "print(sum_greater_than_three(array))  # Fails, because the output shape of `arr[arr > 3.0]` is not inferrable through tracing (without data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec1619",
   "metadata": {},
   "source": [
    "### Impure functions are dangerous with JIT compilation! (Numba & JAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d80a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_sum = False\n",
    "\n",
    "@nb.njit\n",
    "def accumulate(arr):\n",
    "    if do_sum:\n",
    "        return np.sum(arr)\n",
    "    else:\n",
    "        return np.prod(arr)\n",
    "\n",
    "\n",
    "array = np.array([1., 2., 3., 4., 5.])\n",
    "print(\"Accumulate with `np.prod`:\", accumulate(array))\n",
    "\n",
    "# now we switch `do_sum` on!\n",
    "do_sum = True\n",
    "print(\"Accumulate with `np.sum`:\", accumulate(array), f\"...Hey, this should've been {np.sum(array)} instead!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9718f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_sum = False\n",
    "\n",
    "@jax.jit\n",
    "def accumulate(arr):\n",
    "    if do_sum:\n",
    "        return jnp.sum(arr)\n",
    "    else:\n",
    "        return jnp.prod(arr)\n",
    "\n",
    "\n",
    "array = jnp.array([1., 2., 3., 4., 5.])\n",
    "\n",
    "print(\"Accumulate with `jnp.prod`:\", accumulate(array))\n",
    "\n",
    "# now we switch `do_sum` on!\n",
    "do_sum = True\n",
    "print(\"Accumulate with `jnp.sum`:\", accumulate(array), f\"...Hey, this should've been {jnp.sum(array)} instead!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee86c88",
   "metadata": {},
   "source": [
    "We can see why in the JAX case: that's because the traced program _never knew_ that there's a `sum` operation in the first place _and_ the compiled function is cached based on their input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Traced program:\\n\", accumulate.trace(array).jaxpr)\n",
    "print()\n",
    "print(\"HLO program:\\n\", accumulate.lower(array).as_text()) # this is the program that get's compiled by XLA compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e023bd",
   "metadata": {},
   "source": [
    "On to the [project1.ipynb](project1.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959cd834",
   "metadata": {},
   "source": [
    "### Auto-differentiation with JAX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afa1a0",
   "metadata": {},
   "source": [
    "Knowing the computational graph of a program (i.e. JaxPr) gives us the possibility to transform the program. JAX implements different _interpreters_ to execute the JaxPr of which one is able to replace every operation by its gradient:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    return 2.0 + jnp.sin(x)\n",
    "\n",
    "print(\"JaxPr:\")\n",
    "print(jax.make_jaxpr(fun)(1.0))\n",
    "print()\n",
    "\n",
    "grad_fun = jax.grad(fun)\n",
    "print(\"JaxPr (grad):\")\n",
    "print(jax.make_jaxpr(grad_fun)(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec961a58",
   "metadata": {},
   "source": [
    "Gradients are powerful! Many scientific problems involve gradient-based minimizations.\n",
    "\n",
    "Let's implement a gradient-based optimization on our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb959c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.key(42)\n",
    "\n",
    "true_a, true_b = 0.2, 1.1\n",
    "\n",
    "# function that we want to fit\n",
    "@jax.jit\n",
    "def function(x, a, b):\n",
    "    return b*x**2 - 4*a*x - b\n",
    "\n",
    "# generate true data with some noise\n",
    "def generate_data(rng):\n",
    "    x_key, noise_key = jax.random.split(rng)\n",
    "\n",
    "    xs = jax.random.uniform(x_key, (128, 1), minval=-3, maxval=3)\n",
    "    noise = jax.random.normal(noise_key, (128, 1)) * 0.15\n",
    "\n",
    "    ys = function(x=xs + noise, a=true_a, b=true_b)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "# plot data\n",
    "xs, ys = generate_data(rng=rng)\n",
    "plt.scatter(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5545921",
   "metadata": {},
   "source": [
    "We want to know what the true underlying `a` and `b` values are in this distribution. The next cell implements a gradient-based optimization to fit `function` to the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b637ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "# Just a struct that holds the parameters of the function\n",
    "class Params(NamedTuple):\n",
    "    a: jax.Array\n",
    "    b: jax.Array\n",
    "\n",
    "\n",
    "# Initialize parameters for the function (`a` and `b`)\n",
    "def init(rng) -> Params:\n",
    "    a_key, b_key = jax.random.split(rng)\n",
    "    a = jax.random.normal(a_key, ())\n",
    "    b = jax.random.normal(b_key, ())\n",
    "    return Params(a, b)\n",
    "\n",
    "\n",
    "# Compute the loss function (least squares error)\n",
    "def loss(params: Params, x: jax.Array, y: jax.Array) -> jax.Array:\n",
    "    pred = function(x=x, a=params.a, b=params.b)\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "\n",
    "# Perform one gradient descent update step on params using the given data. (~SGD)\n",
    "@jax.jit\n",
    "def update(params: Params, x: jax.Array, y: jax.Array) -> Params:\n",
    "    # Computes the gradients of the loss function with respect to the parameters\n",
    "    grads = jax.grad(loss)(params, x, y)\n",
    "\n",
    "    # Define a step function that updates the parameters\n",
    "    def step(param, grad):\n",
    "      return param - 0.005 * grad  # 0.005 := learning rate\n",
    "\n",
    "    # Apply the step function to each parameter\n",
    "    return jax.tree.map(step, params, grads)\n",
    "\n",
    "\n",
    "# Run the optimization\n",
    "params = init(rng)\n",
    "for _ in range(500):\n",
    "    params = update(params, xs, ys)\n",
    "\n",
    "\n",
    "print(f\"True parameters  : a={true_a:.2f}, b={true_b:.2f}\")\n",
    "print(f\"Fitted parameters: a={params.a:.2f}, b={params.b:.2f}\")\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "pred_ys = function(x=xs, a=params.a, b=params.b)\n",
    "plt.plot(xs, pred_ys, \".\", c='red', label=f'Fit result: a={params.a:.2f}, b={params.b:.2f}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8a09f",
   "metadata": {},
   "source": [
    "This is the key ingredient for training neural networks, see more at tomorrow's ML lecture by Liv!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
